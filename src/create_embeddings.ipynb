{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  page_number  \\\n",
      "0  Caine, 1991. The Effects of Music on the Selec...            0   \n",
      "1  Caine, 1991. The Effects of Music on the Selec...            1   \n",
      "2  Caine, 1991. The Effects of Music on the Selec...            2   \n",
      "3  Caine, 1991. The Effects of Music on the Selec...            3   \n",
      "4  Caine, 1991. The Effects of Music on the Selec...            4   \n",
      "\n",
      "                                                text  n_tokens  \\\n",
      "0  The Effects of Music on the Selected Stress Be...       391   \n",
      "1  Vol. XXVIII. No. 4, Winter, 1991 181 Regardles...       555   \n",
      "2  182 Journal of Music Therapy and Bee (1983) re...       458   \n",
      "3  cations for use of sounds and music are extens...       440   \n",
      "4  184 Journal of Music Therapy Table 1 Group Dem...       515   \n",
      "\n",
      "                                          embeddings  \n",
      "0  [-0.009731687605381012, -0.0012755125062540174...  \n",
      "1  [-0.011171781457960606, -0.0006019424763508141...  \n",
      "2  [-0.027497772127389908, 0.0030994887929409742,...  \n",
      "3  [-0.0037552497815340757, 0.002309629926458001,...  \n",
      "4  [-0.009203528985381126, 0.005502857267856598, ...  \n",
      "<pandas.core.indexing._LocIndexer object at 0x0000021B0BB83880>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import tiktoken\n",
    "import PyPDF2\n",
    "\n",
    "openai.api_key = \"cf0bd49030ed4aa6a6509be1cd9d604b\"\n",
    "openai.api_base = \"https://invuniandesai.openai.azure.com/\"\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15'\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def process_to_txt():\n",
    "\n",
    "    input_directory = \"C:/Users/Marine/Documents/CO_uniandes/2023-2/Tesis_1/MT_papers/1990-1999/\"\n",
    "    files=[]\n",
    "    pages=[]\n",
    "    raw_text = []\n",
    "    \n",
    "    for filename in os.listdir(input_directory):\n",
    "        # Ouvrir le fichier PDF en mode lecture binaire ('rb')\n",
    "        with open(input_directory+filename, 'rb') as pdf_file:\n",
    "            # CrÃ©er un objet PDFReader\n",
    "            try:\n",
    "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "                # Parcourir chaque page du PDF\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    # Extraire le texte de la page\n",
    "                    files.append(filename.split('.pdf')[0])\n",
    "                    pages.append(page_num)\n",
    "                    raw_text.append(pdf_reader.pages[page_num].extract_text())\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(pdf_file,\" \", e)\n",
    "                \n",
    "    data = {'name': files, 'page': pages, 'raw_text': raw_text}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df['n_tokens'] = df.raw_text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "    return df\n",
    "                \n",
    "max_tokens = 1800\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "def split_into_many(text, file_name, pages, max_tokens = max_tokens):\n",
    "    \n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    # Get the number of tokens for each paragraph\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + paragraph)) for paragraph in paragraphs]\n",
    "\n",
    "    chunks = []\n",
    "    file=[]\n",
    "    page=[]\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for paragraph, token in zip(paragraphs, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \" \")\n",
    "            file.append(file_name)\n",
    "            page.append(pages)\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of\n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the paragraph to the chunk and add the number of tokens to the total\n",
    "        chunk.append(paragraph + \" \")\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks, file, page\n",
    "\n",
    "def process_to_embeddings():\n",
    "    shortened = []\n",
    "    files=[]\n",
    "    pages=[]\n",
    "    # Loop through the dataframe\n",
    "    df = process_to_txt()\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        \n",
    "        # If the text is None, go to the next row\n",
    "        if len(row[1]['raw_text'])==1:\n",
    "            continue\n",
    "\n",
    "        # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "        if row[1]['n_tokens'] > max_tokens:\n",
    "            short,file,page = split_into_many(row[1]['raw_text'], row[1]['name'], row[1]['page'])\n",
    "            shortened+=short\n",
    "            files+=file\n",
    "            pages+=page\n",
    "\n",
    "        # Otherwise, add the text to the list of shortened texts\n",
    "        else:\n",
    "            shortened.append( row[1]['raw_text'] )\n",
    "            files.append(row[1]['name'])\n",
    "            pages.append(row[1]['page'])\n",
    "            \n",
    "    data = {'title': files, 'page_number': pages, 'text': shortened}      \n",
    "    df =  pd.DataFrame(data)\n",
    "    df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "    \n",
    "    df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002-rfmanrique')['data'][0]['embedding'])\n",
    "\n",
    "    df.to_csv('embeddings2.csv')\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = process_to_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
