{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "                                               title  page_number  \\\n",
      "0  Bauer et al., 2021. Parental Attitudes toward ...            0   \n",
      "1  Bauer et al., 2021. Parental Attitudes toward ...            1   \n",
      "2  Bauer et al., 2021. Parental Attitudes toward ...            2   \n",
      "3  Bauer et al., 2021. Parental Attitudes toward ...            3   \n",
      "4  Bauer et al., 2021. Parental Attitudes toward ...            4   \n",
      "\n",
      "                                                text  n_tokens  \\\n",
      "0  International  Journal  of \\nEnvironmental Res...      1229   \n",
      "1  Int. J. Environ. Res. Public Health 2021 ,18, ...       855   \n",
      "2  Int. J. Environ. Res. Public Health 2021 ,18, ...       818   \n",
      "3  Int. J. Environ. Res. Public Health 2021 ,18, ...      1145   \n",
      "4  Int. J. Environ. Res. Public Health 2021 ,18, ...       834   \n",
      "\n",
      "                                          embeddings  \n",
      "0  [0.011431924067437649, 0.012565886601805687, 0...  \n",
      "1  [-0.004350075963884592, 0.0003293604822829366,...  \n",
      "2  [-0.003601719858124852, 0.0006005705799907446,...  \n",
      "3  [-0.009698979556560516, 0.02356622740626335, 0...  \n",
      "4  [-0.005119635257869959, 0.007116121705621481, ...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import tiktoken\n",
    "import PyPDF2\n",
    "\n",
    "openai.api_key = \"cf0bd49030ed4aa6a6509be1cd9d604b\"\n",
    "openai.api_base = \"https://invuniandesai.openai.azure.com/\"\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15'\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def process_to_txt():\n",
    "\n",
    "    input_directory = \"C:/Users/Marine/Documents/CO_uniandes/0_Tesis/MT_papers/2020 -_/\"\n",
    "    files=[]\n",
    "    pages=[]\n",
    "    raw_text = []\n",
    "    ok_docs=0\n",
    "    \n",
    "    for root, dirs, filenames in os.walk(input_directory):\n",
    "        for file in filenames:\n",
    "            # Ouvrir le fichier PDF en mode lecture binaire ('rb')\n",
    "            with open(root+'/'+file, 'rb') as pdf_file:\n",
    "                # CrÃ©er un objet PDFReader\n",
    "                try:\n",
    "                    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "                    # Parcourir chaque page du PDF\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        # Extraire le texte de la page\n",
    "                        files.append(file.split('.pdf')[0])\n",
    "                        pages.append(page_num)\n",
    "                        raw_text.append(pdf_reader.pages[page_num].extract_text())\n",
    "                    ok_docs+=1\n",
    "                except Exception as e:\n",
    "                    print(pdf_file,\" \", e)\n",
    "    \n",
    "    print(ok_docs)                \n",
    "    data = {'name': files, 'page': pages, 'raw_text': raw_text}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df['n_tokens'] = df.raw_text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "    return df\n",
    "                \n",
    "max_tokens = 1500\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "def split_into_many(text, file_name, pages, max_tokens = max_tokens):\n",
    "    \n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    # Get the number of tokens for each paragraph\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + paragraph)) for paragraph in paragraphs]\n",
    "\n",
    "    chunks = []\n",
    "    file=[]\n",
    "    page=[]\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for paragraph, token in zip(paragraphs, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \" \")\n",
    "            file.append(file_name)\n",
    "            page.append(pages)\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of\n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the paragraph to the chunk and add the number of tokens to the total\n",
    "        chunk.append(paragraph + \" \")\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks, file, page\n",
    "\n",
    "def emb_with_delay(text):\n",
    "    time.sleep(2)\n",
    "    return openai.Embedding.create(input=text, engine='text-embedding-ada-002-rfmanrique')['data'][0]['embedding']\n",
    "\n",
    "def process_to_embeddings():\n",
    "    shortened = []\n",
    "    files=[]\n",
    "    pages=[]\n",
    "    # Loop through the dataframe\n",
    "    df = process_to_txt()\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        \n",
    "        # If the text is None, go to the next row\n",
    "        if len(row[1]['raw_text'])==1:\n",
    "            continue\n",
    "\n",
    "        # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "        if row[1]['n_tokens'] > max_tokens:\n",
    "            short,file,page = split_into_many(row[1]['raw_text'], row[1]['name'], row[1]['page'])\n",
    "            shortened+=short\n",
    "            files+=file\n",
    "            pages+=page\n",
    "\n",
    "        # Otherwise, add the text to the list of shortened texts\n",
    "        else:\n",
    "            shortened.append( row[1]['raw_text'] )\n",
    "            files.append(row[1]['name'])\n",
    "            pages.append(row[1]['page'])\n",
    "            \n",
    "    data = {'title': files, 'page_number': pages, 'text': shortened}      \n",
    "    df =  pd.DataFrame(data)\n",
    "    df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "    df = df[df['n_tokens']>=10]\n",
    "    df['embeddings'] = df.text.apply(emb_with_delay)\n",
    "        \n",
    "    #df['embeddings'] = df.text.apply(lambda x: openai.Embedding.create(input=x, engine='text-embedding-ada-002-rfmanrique')['data'][0]['embedding'])\n",
    "\n",
    "    df.to_csv('embeddings_20.csv')\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = process_to_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('embeddings.csv', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df70 = pd.read_csv(\"embeddings_70.csv\")\n",
    "df80 = pd.read_csv(\"embeddings_80.csv\")\n",
    "df90 = pd.read_csv(\"embeddings_90.csv\")\n",
    "df00 = pd.read_csv(\"embeddings_00.csv\")\n",
    "df10 = pd.read_csv(\"embeddings_10.csv\")\n",
    "df20 = pd.read_csv(\"embeddings_20.csv\")\n",
    "df_complete = pd.concat([df70,df80,df90,df00,df10,df20])\n",
    "df_complete = df_complete.drop(['Unnamed: 0'], axis=1)\n",
    "df_complete.to_csv('embeddings_complete.csv', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_complete = pd.read_csv(\"embeddings_complete.csv\")\n",
    "df_complete = df_complete.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marine\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "path = \"C:/Users/Marine/Documents/CO_uniandes/0_Tesis/my-music-gpt/src\"\n",
    "chroma_client = chromadb.PersistentClient(path)\n",
    "collection = chroma_client.create_collection(name=\"test_persist\", metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=test_persist)]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "path = \"C:/Users/Marine/Documents/CO_uniandes/0_Tesis/my-music-gpt/src\"\n",
    "chroma_client = chromadb.PersistentClient(path)\n",
    "print(chroma_client.list_collections())\n",
    "collection = chroma_client.get_collection(\"test_persist\")\n",
    "\n",
    "df_complete['embeddings'] = df_complete['embeddings'].apply(eval).apply(np.array)\n",
    "\n",
    "collection.add(\n",
    "            embeddings=[arr.tolist() for arr in df_complete['embeddings'].to_list()],\n",
    "            documents= df_complete['text'].to_list(),\n",
    "            metadatas = df_complete.apply(lambda row: {\"title\": row['title'], \"page\": str(row['page_number']), \"tokens\": str(row['n_tokens'])}, axis=1).tolist(),\n",
    "            ids=[str(i) for i in range(len(df_complete))]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
